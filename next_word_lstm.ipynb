{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description : Next WOrd Prediction Using LSTM\n",
    "### Project Description\n",
    "\n",
    "This project aims to develop a deep learning model for predicting the next word in a given sequence of words.\n",
    "The model is built using LSTM network, which are well-suited for sequence prediction tasks. THe project include  the following steps:\n",
    "\n",
    "1.  Data Collection\n",
    "    we use the text of shakespear's \"Hamlet\" as out dataset .The rich, Complex text provides a good challenge for our model.\n",
    "\n",
    "2. Data Preprocessing : The text data is tokenized, Converted into sequence, and padded to ensure uniform input lengths. THe sequence are then split into training and testing sets.\n",
    "\n",
    "3. Model Building : An LSTM model is constructed with an embedding layer, two LSTM layers and a dense output layer with a softmax activation function to predict the probability of the next word.\n",
    "\n",
    "4. Model Training : The model is trained using the prepared sequence, with early stopping implemented to prevent overfitting. Early stopping monitors the  validation loss and stops training when the loss stops improving.\n",
    "\n",
    "5. Model Evaluation : The Mmodel is evaluated using a set of example sentences to test its ability to predict the next word accurately.\n",
    "\n",
    "6. Deployment : A streamlit web application is developed to allow users to input a sequence of words and get the predicted next word in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Collection\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "data = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "\n",
    "## Save to a file \n",
    "with open('hamlet.txt','w') as file :\n",
    "    file.write(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "import numpy as np \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load the dataset\n",
    "\n",
    "with open('hamlet.txt','r') as file :\n",
    "    text = file.read().lower()\n",
    "\n",
    "## Tokenize the text-creating indexes for words\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index)+1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (617353399.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    for line in text.split('\\n'):\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Create Input Sequences\n",
    "input_sequence = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1,len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequence.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the maximum length\n",
    "max_sequence_length = max([len(x) for x in input_sequence])\n",
    "max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the sequence of the input sequence should be in equall length , so add the padding to it\n",
    "\n",
    "input_sequence = np.array(pad_sequences(input_sequence,maxlen = max_sequence_length,padding = 'pre'))\n",
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating predicitors and label\n",
    "import tensorflow as tf\n",
    "x,y = input_sequence[:,:-1], input_sequence[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into categories\n",
    "y = tf.keras.utils.to_categorical(y,num_classes = total_words)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing  sets\n",
    "x_train , x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train our LSTM RNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words,100,input_length = max_sequence_length-1))\n",
    "model.add(LSTM(150,return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words,activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Complie the model\n",
    "model.compile(loss=\"categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the model\n",
    "history = model.fit(x_train,y_train,epochs = 50, validation_data = (x_test,y_test),verbose =1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the next word\n",
    "def predict_next_word(model, tokenizer, text, max_sequence_len):\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "    if len(token_list) >= max_sequence_len:\n",
    "        token_list = token_list[-(max_sequence_len-1):] # Ensure the sequence length matches max_length\n",
    "    token_list = pad_sequences([token_list],maxlen = max_sequence_len-1,padding='pre')\n",
    "    predicted = model.predict(token_list,verbose =0)\n",
    "    predict_word_index = np.argmax(predicted,axis=1)\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predict_word_index:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"TO be or not to be\"\n",
    "print(f\"Input text: {input_text}\")\n",
    "max_sequence_len =model.input_shape[1]+1\n",
    "next_word = predict_next_word(model,tokenizer, input_text,max_sequence_len)\n",
    "print(f\"The predicted next word is {next_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "model.save(\"next_word_lstm.h5\")\n",
    "\n",
    "\n",
    "## Save the tokenizer\n",
    "import pickle\n",
    "with open('tokenizer.pickle','wb') as handle :\n",
    "    pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ann_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
